{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install git+https://github.com/neurostatslab/tensortools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb23669",
   "metadata": {},
   "source": [
    "# TCA Analysis\n",
    "\n",
    "Duplicate this notebook and experiment away.  \n",
    "See [How to Contribute](../Contributing.md) (steps 4 and 5) for help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f8393",
   "metadata": {},
   "source": [
    "In this notebook (which is based on the third notebook of the [2022 Cosyne tutorial](https://neural-reckoning.github.io/cosyne-tutorial-2022/)), we're going to use surrogate gradient descent to find a solution to the sound localisation problem. The surrogate gradient descent approach and code is heavily inspired by (certainly not stolen) from [Friedemann Zenke's SPyTorch tutorial](https://github.com/fzenke/spytorch), which I recommend for a deeper dive into the maths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")  # Use Apple's Metal Performance Shaders\n",
    "# el\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "my_computer_is_slow = True  # set this to True if using Colab\n",
    "\n",
    "fig_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a4686",
   "metadata": {},
   "source": [
    "## Sound localization stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd312e52",
   "metadata": {},
   "source": [
    "The following function creates a set of stimuli that can be used for training or testing. We have two ears (0 and 1), and ear 1 will get a version of the signal delayed by an IPD we can write as $\\alpha$ in equations (``ipd`` in code). The basic signal is a sine wave as in the previous notebook, made positive, so $(1/2)(1+\\sin(\\theta)$. In addition, for each ear there will be $N_a$ neurons per ear (``anf_per_ear`` because these are auditory nerve fibres). Each neuron generates Poisson spikes at a certain firing rate, and these Poisson spike trains are independent. In addition, since it is hard to train delays, we seed it with uniformly distributed delays from a minimum of 0 to a maximum of $\\pi/2$ in each ear, so that the differences between the two ears can cover the range of possible IPDs ($-\\pi/2$ to $\\pi/2$). We do this directly by adding a phase delay to each neuron. So for ear $i\\in\\{0,1\\}$ and neuron $j$ at time $t$ the angle $\\theta=2\\pi f t+i\\alpha+j\\pi/2N_a$. Finally, we generate Poisson spike trains with a rate $R_\\mathrm{max}((1/2)(1+\\sin(\\theta)))^k$. $R_\\mathrm{max}$ (``rate_max``) is the maximum instantaneous firing rate, and $k$ (``envelope_power``) is a constant that sharpens the envelope. The higher $R_\\mathrm{max}$ and $k$ the easier the problem (try it out on the cell below to see why).\n",
    "\n",
    "Here's a picture of the architecture for the stimuli:\n",
    "\n",
    "![Stimuli architecture](diagrams/arch-stimuli.png)\n",
    "\n",
    "The functions below return two arrays ``ipd`` and ``spikes``. ``ipd`` is an array of length ``num_samples`` that gives the true IPD, and ``spikes`` is an array of 0 (no spike) and 1 (spike) of shape ``(num_samples, duration_steps, 2*anf_per_ear)``, where ``duration_steps`` is the number of time steps there are in the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb26693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using Brian so we just use these constants to make equations look nicer below\n",
    "second = 1\n",
    "ms = 1e-3\n",
    "Hz = 1\n",
    "\n",
    "# Stimulus and simulation parameters\n",
    "dt = 1 * ms  # large time step to make simulations run faster for tutorial\n",
    "anf_per_ear = 100  # repeats of each ear with independent noise\n",
    "envelope_power = 2  # higher values make sharper envelopes, easier\n",
    "rate_max = 600 * Hz  # maximum Poisson firing rate\n",
    "f = 20 * Hz  # stimulus frequency\n",
    "duration = 0.1 * second  # stimulus duration\n",
    "# duration = duration / 2\n",
    "duration_steps = int(np.round(duration / dt))\n",
    "input_size = 2 * anf_per_ear\n",
    "\n",
    "# Generate an input signal (spike array) from array of true IPDs\n",
    "def input_signal(ipd):\n",
    "    num_samples = len(ipd)\n",
    "    T = np.arange(duration_steps) * dt  # array of times\n",
    "    phi = (\n",
    "        2 * np.pi * (f * T + np.random.rand())\n",
    "    )  # array of phases corresponding to those times with random offset\n",
    "    # each point in the array will have a different phase based on which ear it is\n",
    "    # and its delay\n",
    "    theta = np.zeros((num_samples, duration_steps, 2 * anf_per_ear))\n",
    "    # for each ear, we have anf_per_ear different phase delays from to pi/2 so\n",
    "    # that the differences between the two ears can cover the full range from -pi/2 to pi/2\n",
    "    phase_delays = np.linspace(0, np.pi / 2, anf_per_ear)\n",
    "    # now we set up these theta to implement that. Some numpy vectorisation logic here which looks a little weird,\n",
    "    # but implements the idea in the text above.\n",
    "    theta[:, :, :anf_per_ear] = (\n",
    "        phi[np.newaxis, :, np.newaxis] + phase_delays[np.newaxis, np.newaxis, :]\n",
    "    )\n",
    "    theta[:, :, anf_per_ear:] = (\n",
    "        phi[np.newaxis, :, np.newaxis]\n",
    "        + phase_delays[np.newaxis, np.newaxis, :]\n",
    "        + ipd[:, np.newaxis, np.newaxis]\n",
    "    )\n",
    "    # now generate Poisson spikes at the given firing rate as in the previous notebook\n",
    "    spikes = (\n",
    "        np.random.rand(num_samples, duration_steps, 2 * anf_per_ear)\n",
    "        < rate_max * dt * (0.5 * (1 + np.sin(theta))) ** envelope_power\n",
    "    )\n",
    "    return spikes\n",
    "\n",
    "\n",
    "# Generate some true IPDs from U(-pi/2, pi/2) and corresponding spike arrays\n",
    "def random_ipd_input_signal(num_samples, tensor=True):\n",
    "    ipd = (\n",
    "        np.random.rand(num_samples) * np.pi - np.pi / 2\n",
    "    )  # uniformly random in (-pi/2, pi/2)\n",
    "    spikes = input_signal(ipd)\n",
    "    if tensor:\n",
    "        ipd = torch.tensor(ipd, device=device, dtype=dtype)\n",
    "        spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "    return ipd, spikes\n",
    "\n",
    "\n",
    "def random_step_ipd_input_signal(num_samples, tensor=True):\n",
    "    # Generate IPDs linearly spaced from -pi/2 to pi/2\n",
    "    ipd = np.linspace(-np.pi / 2, np.pi / 2, num_samples)\n",
    "    # Generate the corresponding spike arrays\n",
    "    spikes = input_signal(ipd)\n",
    "    if tensor:\n",
    "        ipd = torch.tensor(ipd, device=device, dtype=dtype)\n",
    "        spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "    return ipd, spikes\n",
    "\n",
    "\n",
    "# Plot a few just to show how it looks\n",
    "ipd, spikes = random_ipd_input_signal(8)\n",
    "spikes = spikes.cpu()\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(\n",
    "        spikes[i, :, :].T, aspect=\"auto\", interpolation=\"nearest\", cmap=plt.cm.gray_r\n",
    "    )\n",
    "    plt.title(f\"True IPD = {int(ipd[i]*180/np.pi)} deg\")\n",
    "    if i >= 4:\n",
    "        plt.xlabel(\"Time (steps)\")\n",
    "    if i % 4 == 0:\n",
    "        plt.ylabel(\"Input neuron index\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aab436",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd, spikes = random_step_ipd_input_signal(8)\n",
    "spikes = spikes.cpu()\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(\n",
    "        spikes[i, :, :].T, aspect=\"auto\", interpolation=\"nearest\", cmap=plt.cm.gray_r\n",
    "    )\n",
    "    plt.title(f\"True IPD = {int(ipd[i]*180/np.pi)} deg\")\n",
    "    if i >= 4:\n",
    "        plt.xlabel(\"Time (steps)\")\n",
    "    if i % 4 == 0:\n",
    "        plt.ylabel(\"Input neuron index\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86605734",
   "metadata": {},
   "source": [
    "Now the aim is to take these input spikes and infer the IPD. We can do this either by discretising and using a classification approach, or with a regression approach. For the moment, let's try it with a classification approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beb019",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We train this by dividing the input data into batches and computing gradients across batches. In this notebook, batch and data size is small so that it can be run on a laptop in a couple of minutes, but normally you'd use larger batches and more data. Let's start with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511e30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training. These aren't optimal, but instead designed\n",
    "# to give a reasonable result in a small amount of time for the tutorial!\n",
    "if my_computer_is_slow:\n",
    "    batch_size = 64\n",
    "    n_training_batches = 64\n",
    "else:\n",
    "    batch_size = 128\n",
    "    n_training_batches = 128\n",
    "n_testing_batches = 32\n",
    "num_samples = batch_size * n_training_batches\n",
    "\n",
    "\n",
    "# Generator function iterates over the data in batches\n",
    "# We randomly permute the order of the data to improve learning\n",
    "def data_generator(ipds, spikes, random=True):\n",
    "    if random:\n",
    "        perm = torch.randperm(spikes.shape[0])\n",
    "        spikes = spikes[perm, :, :]\n",
    "        ipds = ipds[perm]\n",
    "    n, _, _ = spikes.shape\n",
    "    n_batch = n // batch_size\n",
    "    for i in range(n_batch):\n",
    "        x_local = spikes[i * batch_size : (i + 1) * batch_size, :, :]\n",
    "        y_local = ipds[i * batch_size : (i + 1) * batch_size]\n",
    "        yield x_local, y_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a735f",
   "metadata": {},
   "source": [
    "## Classification approach\n",
    "\n",
    "We discretise the IPD range of $[-\\pi/2, \\pi/2]$ into $N_c$ (``num_classes``) equal width segments. Replace angle $\\phi$ with the integer part (floor) of $(\\phi+\\pi/2)N_c/\\pi$. We also convert the arrays into PyTorch tensors for later use. The algorithm will now guess the index $i$ of the segment, converting that to the midpoint of the segment $\\phi_i=a+(i+1/2)(b-a)/N_c$ when needed.\n",
    "\n",
    "The algorithm will work by outputting a length $N_c$ vector $y$ and the index of the maximum value of y will be the guess as to the class (1-hot encoding), i.e. $i_\\mathrm{est}=\\mathrm{argmax}_i y_i$. We will perform the training with a softmax and negative loss likelihood loss, which is a standard approach in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f817078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes at 15 degree increments\n",
    "num_classes = 180 // 15\n",
    "print(f\"Number of classes = {num_classes}\")\n",
    "\n",
    "\n",
    "def discretise(ipds):\n",
    "    return ((ipds + np.pi / 2) * num_classes / np.pi).long()  # assumes input is tensor\n",
    "\n",
    "\n",
    "def continuise(ipd_indices):  # convert indices back to IPD midpoints\n",
    "    return (ipd_indices + 0.5) / num_classes * np.pi - np.pi / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b857913",
   "metadata": {},
   "source": [
    "## Membrane only (no spiking neurons)\n",
    "\n",
    "Before we get to spiking, we're going to warm up with a non-spiking network that shows some of the features of the full model but without any coincidence detection, it can't do the task. We basically create a neuron model that has everything except spiking, so the membrane potential dynamics are there and it takes spikes as input. The neuron model we'll use is just the LIF model we've already seen. We'll use a time constant $\\tau$ of 20 ms, and we pre-calculate a constant $\\alpha=\\exp(-dt/\\tau)$ so that updating the membrane potential $v$ is just multiplying by $\\alpha$ (as we saw in the first notebook). We store the input spikes in a vector $s$ of 0s and 1s for each time step, and multiply by the weight matrix $W$ to get the input, i.e. $v\\leftarrow \\alpha v+Ws$.\n",
    "\n",
    "We initialise the weight matrix $W$ uniformly with bounds proportionate to the inverse square root of the number of inputs (fairly standard, and works here).\n",
    "\n",
    "The output of this will be a vector of $N_c$ (``num_classes``) membrane potential traces. We sum these traces over time and use this as the output vector (the largest one will be our prediction of the class and therefore the IPD).\n",
    "\n",
    "![Membrane only architecture](diagrams/arch-membrane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fce85",
   "metadata": {},
   "source": [
    "## Analysis of results\n",
    "\n",
    "Now we compute the training and test accuracy, and plot histograms and confusion matrices to understand the errors it's making.\n",
    "\n",
    "This function evaluates the performance of a classifier on given data.\n",
    "\n",
    "Parameters:\n",
    "ipds (numpy.ndarray): Inter-pulse intervals data.\n",
    "spikes (numpy.ndarray): Spike train data.\n",
    "label (str): Label to be used for the output (e.g., 'Train' or 'Test').\n",
    "run (function): Classifier function to be evaluated.\n",
    "\n",
    "The function works by iterating over the data generated by the `data_generator` function. For each batch of data:\n",
    "- It gets the true labels and discretizes them.\n",
    "- It runs the classifier on the input data.\n",
    "- It sums the classifier's output over the time dimension and finds the class with the highest output.\n",
    "- It calculates the accuracy of the classifier by comparing the predicted classes to the true labels.\n",
    "- It updates the confusion matrix based on the true and predicted classes.\n",
    "- It stores the true and estimated labels, and the accuracy for this batch.\n",
    "\n",
    "After going through all the data, it calculates the overall accuracy and absolute error, and prints them. It also plots two histograms: one for the true labels and one for the estimated labels, and a normalized confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc91c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(ipds, spikes, label, run):\n",
    "    accs = []\n",
    "    ipd_true = []\n",
    "    ipd_est = []\n",
    "    confusion = np.zeros((num_classes, num_classes))\n",
    "    for x_local, y_local in data_generator(ipds, spikes):\n",
    "        y_local_orig = y_local\n",
    "        y_local = discretise(y_local)\n",
    "        output = run(x_local)\n",
    "        m = torch.sum(output, 1)  # Sum time dimension\n",
    "        _, am = torch.max(m, 1)  # argmax over output units\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "        for i, j in zip(y_local.detach().cpu().numpy(), am.detach().cpu().numpy()):\n",
    "            confusion[j, i] += 1\n",
    "        ipd_true.append(y_local_orig.detach().cpu().numpy())\n",
    "        ipd_est.append(continuise(am.detach().cpu().numpy()))\n",
    "        accs.append(tmp)\n",
    "    ipd_true = np.hstack(ipd_true)\n",
    "    ipd_est = np.hstack(ipd_est)\n",
    "    abs_errors_deg = abs(ipd_true - ipd_est) * 180 / np.pi\n",
    "    print()\n",
    "    print(f\"{label} classifier accuracy: {100*np.mean(accs):.1f}%\")\n",
    "    print(f\"{label} absolute error: {np.mean(abs_errors_deg):.1f} deg\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=100)\n",
    "    plt.subplot(121)\n",
    "    plt.hist(ipd_true * 180 / np.pi, bins=num_classes, label=\"True\")\n",
    "    plt.hist(ipd_est * 180 / np.pi, bins=num_classes, label=\"Estimated\")\n",
    "    plt.xlabel(\"IPD\")\n",
    "    plt.yticks([])\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(label)\n",
    "    plt.subplot(122)\n",
    "    confusion /= np.sum(confusion, axis=0)[np.newaxis, :]\n",
    "    plt.imshow(\n",
    "        confusion,\n",
    "        interpolation=\"nearest\",\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        extent=(-90, 90, -90, 90),\n",
    "    )\n",
    "    plt.xlabel(\"True IPD\")\n",
    "    plt.ylabel(\"Estimated IPD\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return 100 * np.mean(accs)\n",
    "\n",
    "\n",
    "# print(f\"Chance accuracy level: {100*1/num_classes:.1f}%\")\n",
    "# run_func = lambda x: membrane_only(x, W)\n",
    "# analyse(ipds, spikes, 'Train', run=run_func)\n",
    "# ipds_test, spikes_test = random_ipd_input_signal(batch_size*n_testing_batches)\n",
    "# analyse(ipds_test, spikes_test, 'Test', run=run_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b0bbf",
   "metadata": {},
   "source": [
    "This poor performance isn't surprising because this network is not actually doing any coincidence detection, just a weighted sum of input spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb91016",
   "metadata": {},
   "source": [
    "## Spiking model\n",
    "\n",
    "Next we'll implement a version of the model with spikes to see how that changes performance. We'll just add a single hidden feed-forward layer of spiking neurons between the input and the output layers. This layer will be spiking, so we need to use the surrogate gradient descent approach.\n",
    "\n",
    "![Full architecture](diagrams/arch-full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5456e",
   "metadata": {},
   "source": [
    "### Surrogate gradient descent\n",
    "\n",
    "First, this is the key part of surrogate gradient descent, a function where we override the computation of the gradient to replace it with a smoothed gradient. You can see that in the forward pass (method ``forward``) it returns the Heaviside function of the input (takes value 1 if the input is ``>0``) or value 0 otherwise. In the backwards pass, it returns the gradient of a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5fabc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 5\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (input,) = ctx.saved_tensors\n",
    "        # Original SPyTorch/SuperSpike gradient\n",
    "        # This seems to be a typo or error? But it works well\n",
    "        # grad = grad_output/(100*torch.abs(input)+1.0)**2\n",
    "        # Sigmoid\n",
    "        grad = (\n",
    "            grad_output\n",
    "            * beta\n",
    "            * torch.sigmoid(beta * input)\n",
    "            * (1 - torch.sigmoid(beta * input))\n",
    "        )\n",
    "        return grad\n",
    "\n",
    "\n",
    "spike_fn = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911318ee",
   "metadata": {},
   "source": [
    "### Updated model\n",
    "\n",
    "The code for the updated model is very similar to the membrane only layer. First, for initialisation we now need two weight matrices, $W_1$ from the input to the hidden layer, and $W_2$ from the hidden layer to the output layer. Second, we run two passes of the loop that you saw above for the membrane only model.\n",
    "\n",
    "The first pass computes the output spikes of the hidden layer. The second pass computes the output layer and is exactly the same as before except using the spikes from the hidden layer instead of the input layer.\n",
    "\n",
    "For the first pass, we modify the function in two ways.\n",
    "\n",
    "Firstly, we compute the spikes with the line ``s = spike_fn(v-1)``. In the forward pass this just computes the Heaviside function of $v-1$, i.e. returns 1 if $v>1$, otherwise 0, which is the spike threshold function for the LIF neuron. In the backwards pass, it returns a gradient of the smoothed version of the Heaviside function.\n",
    "\n",
    "The other line we change is the membrane potential update line. Now, we multiply by $1-s$ where ($s=1$ if there was a spike in the previous time step, otherwise $s=0$), so that the membrane potential is reset to 0 after a spike (but in a differentiable way rather than just setting it to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b072bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 30\n",
    "\n",
    "\n",
    "# Weights and uniform weight initialisation\n",
    "def init_weight_matrices():\n",
    "    # Input to hidden layer\n",
    "    W1 = nn.Parameter(\n",
    "        torch.empty(\n",
    "            (input_size, num_hidden), device=device, dtype=dtype, requires_grad=True\n",
    "        )\n",
    "    )\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W1)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(W1, -bound, bound)\n",
    "    # Hidden layer to output\n",
    "    W2 = nn.Parameter(\n",
    "        torch.empty(\n",
    "            (num_hidden, num_classes), device=device, dtype=dtype, requires_grad=True\n",
    "        )\n",
    "    )\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W2)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(W2, -bound, bound)\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "def snn(input_spikes, W1, W2, tau=20 * ms):\n",
    "    # First layer: input to hidden\n",
    "    v = torch.zeros((batch_size, num_hidden), device=device, dtype=dtype)\n",
    "    s = torch.zeros((batch_size, num_hidden), device=device, dtype=dtype)\n",
    "    s_rec = [s]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (input_spikes, W1))\n",
    "    alpha = np.exp(-dt / tau)\n",
    "    for t in range(duration_steps - 1):\n",
    "        new_v = (alpha * v + h[:, t, :]) * (1 - s)  # multiply by 0 after a spike\n",
    "        s = spike_fn(v - 1)  # threshold of 1\n",
    "        v = new_v\n",
    "        s_rec.append(s)\n",
    "    s_rec = torch.stack(s_rec, dim=1)\n",
    "    # Second layer: hidden to output\n",
    "    v = torch.zeros((batch_size, num_classes), device=device, dtype=dtype)\n",
    "    s = torch.zeros((batch_size, num_classes), device=device, dtype=dtype)\n",
    "    v_rec = [v]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (s_rec, W2))\n",
    "    alpha = np.exp(-dt / tau)\n",
    "    for t in range(duration_steps - 1):\n",
    "        v = alpha * v + h[:, t, :]\n",
    "        v_rec.append(v)\n",
    "    v_rec = torch.stack(v_rec, dim=1)\n",
    "    # Return recorded spike trains and membrane potentials\n",
    "    return s_rec, v_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1662e0",
   "metadata": {},
   "source": [
    "### Training and analysing\n",
    "\n",
    "We train it as before, except that we modify the functions to take the two weight matrices into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e65ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with spike recording, including network output IPDs\n",
    "def train_network(ipds, spikes, nb_epochs, lr, num_classes):\n",
    "    W1, W2 = init_weight_matrices()\n",
    "    optimizer = torch.optim.Adam([W1, W2], lr=lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    spike_data = []\n",
    "    input_ipd_data = []\n",
    "    estimated_ipd_data = []\n",
    "    loss_hist = []\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in data_generator(discretise(ipds), spikes):\n",
    "            # Run network\n",
    "            output = snn(x_local, W1, W2)\n",
    "            v_rec = output[-1]\n",
    "            s_rec = output[0]\n",
    "\n",
    "            # Record spikes and corresponding IPD values\n",
    "            spike_data.append(s_rec.detach().cpu().numpy())  # Detach and move to CPU\n",
    "            input_ipd_data.append(y_local.detach().cpu().numpy())\n",
    "\n",
    "            # Compute cross entropy loss\n",
    "            m = torch.mean(v_rec, 1)  # Mean across time dimension\n",
    "            loss = loss_fn(log_softmax_fn(m), y_local)\n",
    "            local_loss.append(loss.item())\n",
    "\n",
    "            # Record estimated IPDs\n",
    "            _, estimated_ipds = torch.max(log_softmax_fn(m), 1)\n",
    "            estimated_ipd_data.append(estimated_ipds.detach().cpu().numpy())\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_hist.append(np.mean(local_loss))\n",
    "        print(\"Epoch %i: loss=%.5f\" % (e + 1, np.mean(local_loss)))\n",
    "\n",
    "    # Convert lists of data to tensors\n",
    "    spike_tensor = torch.tensor(np.array(spike_data)).float()\n",
    "    input_ipd_tensor = torch.tensor(np.array(input_ipd_data)).float()\n",
    "    estimated_ipd_tensor = torch.tensor(np.array(estimated_ipd_data)).float()\n",
    "\n",
    "    return W1, W2, spike_tensor, input_ipd_tensor, estimated_ipd_tensor\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "nb_epochs = 10  # quick, it won't have converged\n",
    "lr = 0.01\n",
    "\n",
    "# Generate the training data\n",
    "ipds, spikes = random_ipd_input_signal(num_samples)\n",
    "W1, W2, recorded_spikes, recorded_input_ipds, recorded_estimated_ipds = train_network(\n",
    "    ipds, spikes, nb_epochs, lr, num_classes\n",
    ")\n",
    "\n",
    "print(\"Recorded spikes shape: \", recorded_spikes.shape)\n",
    "print(\"Recorded input IPDs shape: \", recorded_input_ipds.shape)\n",
    "print(\"Recorded estimated IPDs shape: \", recorded_estimated_ipds.shape)\n",
    "# Analyse\n",
    "print(f\"Chance accuracy level: {100*1/num_classes:.1f}%\")\n",
    "run_func = lambda x: snn(x, W1, W2)[-1]\n",
    "analyse(ipds, spikes, \"Train\", run=run_func)\n",
    "ipds_test, spikes_test = random_ipd_input_signal(batch_size * n_testing_batches)\n",
    "analyse(ipds_test, spikes_test, \"Test\", run=run_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281d690",
   "metadata": {},
   "source": [
    "## TCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a816483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortools.cpwarp import ShiftedCP, fit_shifted_cp\n",
    "import tensortools as tt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "n_trials = 100  # 1\n",
    "N_RESTARTS = 5\n",
    "MAX_SHIFT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8818a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_with_ipd_coloring(\n",
    "    m, ipds, titles=(\"Trial\", \"Neuron\", \"Time\"), vertical_layout=True\n",
    "):\n",
    "    num_ranks = len(m.factors[0])\n",
    "\n",
    "    colors = plt.cm.get_cmap(\"tab20\", num_ranks)\n",
    "\n",
    "    if vertical_layout:\n",
    "        n_rows = num_ranks\n",
    "        n_cols = len(m.factors)\n",
    "    else:\n",
    "        n_rows = len(m.factors)\n",
    "        n_cols = num_ranks\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 2 * n_rows))\n",
    "    if n_rows * n_cols == 1:\n",
    "        axes = np.array([[axes]])  # Double bracket to make it 2D\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.reshape(n_rows, n_cols)  # Ensure axes is always a 2D array\n",
    "\n",
    "    def normalize(f):\n",
    "        return f / np.linalg.norm(f)\n",
    "\n",
    "    # Plot each factor in each mode\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax = axes[i, j]\n",
    "            factor = m.factors[j][i] if vertical_layout else m.factors[i][j]\n",
    "            norm_factor = normalize(factor)\n",
    "\n",
    "            if j == 0:  # Trial factors\n",
    "                # Scatter plot for trial factors\n",
    "                scatter = ax.scatter(\n",
    "                    range(len(norm_factor)),\n",
    "                    norm_factor,\n",
    "                    c=continuise(ipds),\n",
    "                    cmap=\"viridis\",\n",
    "                    edgecolor=\"k\",\n",
    "                )\n",
    "                fig.colorbar(scatter, ax=ax, label=\"IPD\")\n",
    "                ax.set_title(f\"{titles[j]} Factor {i+1}\")\n",
    "            elif j == 1:  # Neuron factors\n",
    "                # Bar plot for neuron factors\n",
    "                ax.bar(range(len(norm_factor)), norm_factor, color=\"blue\")\n",
    "                ax.set_title(f\"{titles[j]} Factor {i+1}\")\n",
    "            else:  # Time factors\n",
    "                # Line plot for time factors\n",
    "                ax.plot(norm_factor, color=colors(i))\n",
    "                ax.set_title(f\"{titles[j]} Factor {i+1}\")\n",
    "\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "            if i == n_rows - 1:\n",
    "                ax.set_xlabel(titles[j], labelpad=0)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(\"Activation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605a750",
   "metadata": {},
   "source": [
    "### During training analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11777b35",
   "metadata": {},
   "source": [
    "This tensor represents the recorded spikes across 640 iterations (10 epochs × 64 batches per epoch).\n",
    "640: Represents each batch processed across all epochs.\n",
    "64: Each entry within the batch represents one sample, and there are 64 samples per batch.\n",
    "100: Represents the duration steps, i.e., the number of time steps for which the neural activity is recorded (100 ms in your setup).\n",
    "30: Represents the number of neurons in the hidden layer for which you're recording spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spikes(recorded_spikes, sigma=2.0):\n",
    "    # convert to NumPy array\n",
    "    if isinstance(recorded_spikes, torch.Tensor):\n",
    "        recorded_spikes = recorded_spikes.cpu().numpy().astype(np.float64)\n",
    "\n",
    "    # collapse the first two dimensions (epochs*batches and batch_size)\n",
    "    reshaped_spikes = recorded_spikes.reshape(\n",
    "        -1, recorded_spikes.shape[2], recorded_spikes.shape[3]\n",
    "    )\n",
    "\n",
    "    # Change from [total_samples, time, neurons] to [total_samples, neurons, time]\n",
    "    reshaped_spikes = np.transpose(reshaped_spikes, (0, 2, 1))\n",
    "\n",
    "    # Apply Gaussian smoothing along the time axis for each neuron\n",
    "    smoothed_spikes = np.zeros_like(reshaped_spikes)\n",
    "    for i in range(reshaped_spikes.shape[0]):\n",
    "        for j in range(reshaped_spikes.shape[1]):\n",
    "            smoothed_spikes[i, j, :] = gaussian_filter1d(\n",
    "                reshaped_spikes[i, j, :], sigma=sigma\n",
    "            )\n",
    "\n",
    "    return smoothed_spikes\n",
    "\n",
    "\n",
    "# Convert the recorded spikes into a reshaped tensor\n",
    "spikes_tensor = transform_spikes(recorded_spikes)\n",
    "print(\"Shape of spikes_tensor:\", spikes_tensor.shape)\n",
    "# This will reshape [640, 64] to [40960]\n",
    "recorded_input_ipds = recorded_input_ipds.view(-1)\n",
    "recorded_ipds = recorded_input_ipds.detach().cpu().numpy().astype(np.float64)\n",
    "recorded_estimated_ipds = (\n",
    "    recorded_estimated_ipds.view(-1).detach().cpu().numpy().astype(np.float64)\n",
    ")\n",
    "recorded_ipds.flatten()\n",
    "recorded_estimated_ipds.flatten()\n",
    "print(\"Shape of recorded_ipds:\", recorded_input_ipds.shape)\n",
    "print(\"Shape of recorded_estimated_ipds:\", recorded_estimated_ipds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate\n",
    "divide_by = 500\n",
    "\n",
    "# Sample every 'divide_by' sample across all tensors\n",
    "spikes_tensor = spikes_tensor[::divide_by]\n",
    "recorded_ipds = recorded_ipds[::divide_by]\n",
    "recorded_estimated_ipds = recorded_estimated_ipds[::divide_by]\n",
    "\n",
    "# Print the shapes to confirm the operation\n",
    "print(\"Shape of spikes_tensor:\", spikes_tensor.shape)\n",
    "print(\"Shape of recorded_ipds:\", recorded_ipds.shape)\n",
    "print(\"Shape of recorded_estimated_ipds:\", recorded_estimated_ipds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48289227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum num of components using reconstruction error\n",
    "num_components = 15\n",
    "\n",
    "# Fit an ensemble of models, 4 random replicates / optimization runs per model rank\n",
    "ensemble = tt.Ensemble(fit_method=\"ncp_hals\")\n",
    "ensemble.fit(spikes_tensor, ranks=range(1, num_components), replicates=5)  # range(1,32)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "# plot reconstruction error as a function of num components.\n",
    "tt.plot_objective(ensemble, ax=axes[0])\n",
    "# plot model similarity as a function of num components.\n",
    "tt.plot_similarity(ensemble, ax=axes[1])\n",
    "fig.tight_layout()\n",
    "\n",
    "# Plot the low-d factors\n",
    "replicate = 0\n",
    "tt.plot_factors(ensemble.factors(num_components - 1)[replicate])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a370ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 6\n",
    "\n",
    "during_training_model = fit_shifted_cp(\n",
    "    spikes_tensor,\n",
    "    rank=rank,\n",
    "    boundary=\"wrap\",\n",
    "    # n_restarts=N_RESTARTS,\n",
    "    n_restarts=10,\n",
    "    max_shift_axis0=MAX_SHIFT,\n",
    "    max_shift_axis1=None,\n",
    "    max_iter=100,\n",
    "    u_nonneg=True,\n",
    "    v_nonneg=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_result_with_ipd_coloring(\n",
    "    during_training_model, recorded_ipds, vertical_layout=True\n",
    ")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20eff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1 model\n",
    "during_training_model1 = fit_shifted_cp(\n",
    "    spikes_tensor,\n",
    "    rank=1,\n",
    "    boundary=\"wrap\",\n",
    "    # n_restarts=N_RESTARTS,\n",
    "    n_restarts=10,\n",
    "    max_shift_axis0=MAX_SHIFT,\n",
    "    max_shift_axis1=None,\n",
    "    max_iter=100,\n",
    "    u_nonneg=True,\n",
    "    v_nonneg=True,\n",
    ")\n",
    "\n",
    "fig = plot_result_with_ipd_coloring(\n",
    "    during_training_model1, recorded_ipds, vertical_layout=True\n",
    ")\n",
    "# fig.suptitle(\"Shifted Tensor Decomposition\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dd924",
   "metadata": {},
   "source": [
    "### Evaluation post-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single IPD dataset\n",
    "def non_random_ipd_input_signal(num_samples, bin=1, tensor=True):\n",
    "    # ipd = (\n",
    "    #     np.random.rand(num_samples) * np.pi - np.pi / 2\n",
    "    # )  # uniformly random in (-pi/2, pi/2)\n",
    "    # generate a set of ipds that are all the same\n",
    "    # ipd = np.repeat(ipd, num_samples)\n",
    "    # generate ipds that fall within the class range\n",
    "    ipd = continuise(bin)\n",
    "    ipd = np.repeat(ipd, num_samples)\n",
    "    # add noise to ipds of 7.5 degrees\n",
    "    ipd = ipd + np.random.uniform(-np.pi / 24, np.pi / 24, size=ipd.shape)\n",
    "\n",
    "    spikes = input_signal(ipd)\n",
    "    if tensor:\n",
    "        ipd = torch.tensor(ipd, device=device, dtype=dtype)\n",
    "        spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "    return ipd, spikes\n",
    "\n",
    "\n",
    "# Plot a few just to show how it looks\n",
    "# bins range from 0 - 11\n",
    "bin = 5\n",
    "ipd, spikes = non_random_ipd_input_signal(8, bin=11)  # 6 is 0+15 degrees\n",
    "spikes_plot = spikes.cpu()\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(\n",
    "        spikes_plot[i, :, :].T,\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=plt.cm.gray_r,\n",
    "    )\n",
    "    plt.title(f\"True IPD = {int(ipd[i]*180/np.pi)} deg\")\n",
    "    if i >= 4:\n",
    "        plt.xlabel(\"Time (steps)\")\n",
    "    if i % 4 == 0:\n",
    "        plt.ylabel(\"Input neuron index\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_ipd_range(ipds, spikes, model):\n",
    "    \"\"\"\n",
    "    This function evaluates a model across the full range of IPD values and collects the output spikes\n",
    "    and estimated IPDs.\n",
    "    \"\"\"\n",
    "    spike_data = []\n",
    "    ipd_data = []\n",
    "    estimated_ipd_data = []\n",
    "\n",
    "    # Generate data in batches and evaluate\n",
    "    for x_local, y_local in data_generator(discretise(ipds), spikes, random=False):\n",
    "        # Run the model to get output spikes and membrane potentials\n",
    "        output = model(x_local)\n",
    "        output_spikes, output_vrec = output\n",
    "\n",
    "        # Compute the estimated IPDs from the output membrane potentials\n",
    "        _, estimated_ipds = torch.max(\n",
    "            output_vrec, dim=1\n",
    "        )  # Ensure you specify the dimension if needed\n",
    "\n",
    "        # get the estimated ipds as a list of categories\n",
    "        estimated_ipds = estimated_ipds.cpu().detach().numpy()\n",
    "        # convert from [4096, 12] to [4096]\n",
    "        estimated_ipds = np.argmax(estimated_ipds, axis=1)\n",
    "\n",
    "        # Collect output spikes, true IPD values, and estimated IPD values\n",
    "        spike_data.append(output_spikes.detach().cpu().numpy())\n",
    "        ipd_data.append(y_local.detach().cpu().numpy())\n",
    "        estimated_ipd_data.append(estimated_ipds)\n",
    "\n",
    "    # Convert list to tensors\n",
    "    spikes_tensor = torch.tensor(np.concatenate(spike_data), dtype=torch.float32)\n",
    "    ipd_tensor = torch.tensor(np.concatenate(ipd_data), dtype=torch.float32)\n",
    "    estimated_ipd_tensor = torch.tensor(\n",
    "        np.concatenate(estimated_ipd_data), dtype=torch.int64\n",
    "    )\n",
    "\n",
    "    return spikes_tensor, ipd_tensor, estimated_ipd_tensor\n",
    "\n",
    "\n",
    "print(f\"Chance accuracy level: {100*1/num_classes:.1f}%\")\n",
    "snn_model = lambda x: snn(x, W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae48c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single IPD\n",
    "# ipd, spikes = non_random_ipd_input_signal(num_samples, bin=bin)\n",
    "# Normal random IPD\n",
    "# ipd, spikes = random_ipd_input_signal(num_samples)\n",
    "# Ordered IPDs\n",
    "ipd, spikes = random_step_ipd_input_signal(num_samples)\n",
    "\n",
    "print(\"spikes shape: \", spikes.shape)\n",
    "output_spikes_test, output_ipds_test, output_est_ipds = evaluate_full_ipd_range(\n",
    "    ipd, spikes, snn_model\n",
    ")\n",
    "\n",
    "print(\"Output spikes test shape:\", output_spikes_test.shape)\n",
    "print(\"Output IPDs test shape:\", output_ipds_test.shape)\n",
    "print(\"Output estimated IPDs shape:\", output_est_ipds.shape)\n",
    "\n",
    "output_ipds_test_pca = output_ipds_test.detach().cpu().numpy().astype(np.float64)\n",
    "output_est_ipds_pca = output_est_ipds.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "# create a vecctor with 0 and 1 for correct and incorrect predictions\n",
    "correct_predictions = (output_ipds_test_pca == output_est_ipds_pca).astype(int)\n",
    "incorrect_predictions = (output_ipds_test_pca != output_est_ipds_pca).astype(int)\n",
    "print(\"Number of correct predictions:\", np.sum(correct_predictions))\n",
    "print(\"Number of incorrect predictions:\", np.sum(incorrect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4ec6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_eval_spikes(recorded_spikes, sigma=2.0):\n",
    "    # Convert to num samples, neurons, time\n",
    "    reshaped_spikes = recorded_spikes.permute(0, 2, 1).cpu().numpy().astype(np.float64)\n",
    "\n",
    "    # Apply Gaussian smoothing along the time axis for each neuron\n",
    "    smoothed_spikes = np.zeros_like(reshaped_spikes)\n",
    "    for i in range(reshaped_spikes.shape[0]):\n",
    "        for j in range(reshaped_spikes.shape[1]):\n",
    "            smoothed_spikes[i, j, :] = gaussian_filter1d(\n",
    "                reshaped_spikes[i, j, :], sigma=sigma\n",
    "            )\n",
    "\n",
    "    return smoothed_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_spikes_test = transform_eval_spikes(output_spikes_test)\n",
    "\n",
    "output_ipds_test = (\n",
    "    output_ipds_test.view(-1).detach().cpu().numpy().astype(np.float64).flatten()\n",
    ")\n",
    "print(\"Output spikes test shape:\", output_spikes_test.shape)\n",
    "print(\"Output IPDs test shape:\", output_ipds_test.shape)\n",
    "\n",
    "# Reduce the size of the spikes_tensor, recorded_ipds and recorded_estimated_ipds by sampling every 10th sample\n",
    "divide_by = 10\n",
    "output_spikes_test = output_spikes_test[::divide_by]\n",
    "output_ipds_test = output_ipds_test[::divide_by]\n",
    "\n",
    "print(\"Output spikes test shape:\", output_spikes_test.shape)\n",
    "print(\"Output IPDs test shape:\", output_ipds_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test up to 18 components\n",
    "num_components = 18\n",
    "\n",
    "# Fit an ensemble of models, 4 random replicates / optimization runs per model rank\n",
    "ensemble = tt.Ensemble(fit_method=\"ncp_hals\")\n",
    "ensemble.fit(\n",
    "    output_spikes_test, ranks=range(1, num_components), replicates=5\n",
    ")  # range(1,32)\n",
    "# list of numbers from 1 to 40 in steps of 5 range(7, 10, 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "# plot reconstruction error as a function of num components.\n",
    "tt.plot_objective(ensemble, ax=axes[0])\n",
    "# plot model similarity as a function of num components.\n",
    "tt.plot_similarity(ensemble, ax=axes[1])\n",
    "fig.tight_layout()\n",
    "\n",
    "# Plot the low-d factors =\n",
    "replicate = 0\n",
    "tt.plot_factors(\n",
    "    ensemble.factors(num_components - 1)[replicate]\n",
    ")  # plot the low-d factors\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87655f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testing_model = fit_shifted_cp(\n",
    "    output_spikes_test,\n",
    "    rank=1,\n",
    "    boundary=\"wrap\",\n",
    "    n_restarts=N_RESTARTS,\n",
    "    max_shift_axis0=MAX_SHIFT,\n",
    "    max_shift_axis1=None,\n",
    "    max_iter=100,\n",
    "    u_nonneg=False,\n",
    "    v_nonneg=True,\n",
    ")\n",
    "\n",
    "# Plot a few raster plots of output_spikes_test ([4096, 100, 30])\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(\n",
    "        output_spikes_test[i, :, :],\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=plt.cm.gray_r,\n",
    "    )\n",
    "    axs[i].set_title(f\"True IPD = {output_ipds_test[i]} deg\")\n",
    "    axs[i].set_xlabel(\"Time (steps)\")\n",
    "    axs[i].set_ylabel(\"Output neuron index\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Example usage assuming model is defined and appropriate\n",
    "fig = plot_result_with_ipd_coloring(\n",
    "    eval_testing_model, output_ipds_test, vertical_layout=True\n",
    ")\n",
    "# fig.suptitle(\"Shifted Tensor Decomposition\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testing_model = fit_shifted_cp(\n",
    "    output_spikes_test,\n",
    "    rank=rank,\n",
    "    boundary=\"wrap\",\n",
    "    n_restarts=N_RESTARTS,\n",
    "    max_shift_axis0=MAX_SHIFT,\n",
    "    max_shift_axis1=None,\n",
    "    max_iter=100,\n",
    "    u_nonneg=False,\n",
    "    v_nonneg=True,\n",
    ")\n",
    "\n",
    "# Plot a few raster plots of output_spikes_test ([4096, 100, 30])\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(\n",
    "        output_spikes_test[i, :, :],\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=plt.cm.gray_r,\n",
    "    )\n",
    "    axs[i].set_title(f\"True IPD = {output_ipds_test[i]} deg\")\n",
    "    axs[i].set_xlabel(\"Time (steps)\")\n",
    "    axs[i].set_ylabel(\"Output neuron index\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Example usage assuming model is defined and appropriate\n",
    "fig = plot_result_with_ipd_coloring(\n",
    "    eval_testing_model, output_ipds_test, vertical_layout=True\n",
    ")\n",
    "# fig.suptitle(\"Shifted Tensor Decomposition\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_and_evaluate(ipds, spikes, nb_epochs, lr, num_classes, rank, batch_function):\n",
    "    W1, W2, recorded_spikes, recorded_input_ipds, recorded_estimated_ipds = (\n",
    "        train_network(ipds, spikes, nb_epochs, lr, num_classes)\n",
    "    )\n",
    "    spikes_tensor = transform_spikes(recorded_spikes)\n",
    "    spikes_tensor = spikes_tensor[::500]\n",
    "    during_training_model = fit_shifted_cp(\n",
    "        spikes_tensor,\n",
    "        rank=rank,\n",
    "        boundary=\"wrap\",\n",
    "        n_restarts=10,\n",
    "        max_shift_axis0=MAX_SHIFT,\n",
    "        max_iter=100,\n",
    "        u_nonneg=True,\n",
    "        v_nonneg=True,\n",
    "    )\n",
    "    output_spikes_test, output_ipds_test, output_est_ipds = evaluate_full_ipd_range(\n",
    "        ipds, spikes, lambda x: snn(x, W1, W2)\n",
    "    )\n",
    "    output_spikes_test = transform_eval_spikes(output_spikes_test)[::10]\n",
    "    eval_testing_model = fit_shifted_cp(\n",
    "        output_spikes_test,\n",
    "        rank=rank,\n",
    "        boundary=\"wrap\",\n",
    "        n_restarts=10,\n",
    "        max_shift_axis0=MAX_SHIFT,\n",
    "        max_iter=100,\n",
    "        u_nonneg=False,\n",
    "        v_nonneg=True,\n",
    "    )\n",
    "    train_accuracy = analyse(ipds, spikes, \"Train\", lambda x: snn(x, W1, W2)[-1])\n",
    "    ipds_test, spikes_test = random_ipd_input_signal(num_samples)\n",
    "    test_accuracy = analyse(\n",
    "        ipds_test, spikes_test, \"Test\", lambda x: snn(x, W1, W2)[-1]\n",
    "    )\n",
    "\n",
    "    return (during_training_model, eval_testing_model, train_accuracy, test_accuracy)\n",
    "\n",
    "\n",
    "def run_multiple_experiments(\n",
    "    num_trials, ipds, spikes, nb_epochs, lr, num_classes, rank, batch_function\n",
    "):\n",
    "    results = []\n",
    "    accuracies = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                train_and_evaluate,\n",
    "                ipds,\n",
    "                spikes,\n",
    "                nb_epochs,\n",
    "                lr,\n",
    "                num_classes,\n",
    "                rank,\n",
    "                batch_function,\n",
    "            )\n",
    "            for _ in range(num_trials)\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result[:2])  # Store models\n",
    "                accuracies.append(\n",
    "                    {\"Train Accuracy\": result[2], \"Test Accuracy\": result[3]}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                # Optionally handle the failed case or retry\n",
    "\n",
    "    df_accuracies = pd.DataFrame(accuracies)\n",
    "    return results, df_accuracies\n",
    "\n",
    "\n",
    "nb_epochs = 10\n",
    "lr = 0.01\n",
    "rank = 1\n",
    "num_samples = 1000\n",
    "ipds, spikes = random_ipd_input_signal(num_samples)\n",
    "results, accuracies = run_multiple_experiments(\n",
    "    50, ipds, spikes, nb_epochs, lr, num_classes, rank, data_generator\n",
    ")\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c726938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][0].factors[0].shape  # trial factors\n",
    "# # results[0][0].factors[1].shape # neuron factors\n",
    "# # results[0][0].factors[2].shape # time factors\n",
    "\n",
    "# models = [result[0] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the during training models for analysis\n",
    "models = [result[0] for result in results]\n",
    "\n",
    "if models:\n",
    "    print(\"Shape of trial factors:\", models[0].factors[0].shape)  # trial factors\n",
    "    print(\"Shape of neuron factors:\", models[0].factors[1].shape)  # neuron factors\n",
    "    print(\"Shape of time factors:\", models[0].factors[2].shape)  # time factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "\n",
    "def plot_factors(models, normalize_temporal=False):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "    temporal_data_all = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        color = plt.cm.viridis(i / len(models))\n",
    "\n",
    "        # Normalize trial data\n",
    "        trial_data = normalize_data(model.factors[0][0, :])\n",
    "        axes[0].plot(trial_data, label=f\"Model {i+1}\", color=color)\n",
    "\n",
    "        # Normalize neuron data\n",
    "        neuron_data = normalize_data(model.factors[1][0, :])\n",
    "        axes[1].bar(\n",
    "            np.arange(len(neuron_data)),\n",
    "            neuron_data,\n",
    "            label=f\"Model {i+1}\",\n",
    "            color=color,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        # temporal data\n",
    "        temporal_data = model.factors[2][0, :]\n",
    "        if normalize_temporal:\n",
    "            temporal_data = normalize_data(temporal_data)\n",
    "        axes[2].plot(temporal_data, label=f\"Model {i+1}\", color=color)\n",
    "\n",
    "        # shading\n",
    "        temporal_data_all.append(temporal_data)\n",
    "\n",
    "    temporal_mean = np.mean(temporal_data_all, axis=0)\n",
    "    temporal_std = np.std(temporal_data_all, axis=0)\n",
    "\n",
    "    axes[2].fill_between(\n",
    "        range(len(temporal_mean)),\n",
    "        temporal_mean - temporal_std,\n",
    "        temporal_mean + temporal_std,\n",
    "        color=\"gray\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "\n",
    "    axes[0].set_title(\"Normalized Trial Factors\")\n",
    "    axes[1].set_title(\"Normalized Neuron Factors (Bar)\")\n",
    "    axes[2].set_title(\"Temporal Factors with Shading\")\n",
    "\n",
    "    # for ax in axes:\n",
    "    #     ax.legend(loc=\"upper right\")\n",
    "    # ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "plot_factors(models, normalize_temporal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temporal_factors(models, normalize_temporal=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    temporal_data_all = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        color = plt.cm.viridis(i / len(models))\n",
    "\n",
    "        temporal_data = model.factors[2][0, :]\n",
    "        if normalize_temporal:\n",
    "            temporal_data = normalize_data(temporal_data)\n",
    "        ax.plot(temporal_data, label=f\"Model {i+1}\", color=color, alpha=0.4)\n",
    "\n",
    "        temporal_data_all.append(temporal_data)\n",
    "\n",
    "    temporal_mean = np.mean(temporal_data_all, axis=0)\n",
    "    temporal_std = np.std(temporal_data_all, axis=0)\n",
    "\n",
    "    ax.fill_between(\n",
    "        range(len(temporal_mean)),\n",
    "        temporal_mean - temporal_std,\n",
    "        temporal_mean + temporal_std,\n",
    "        color=\"gray\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    # ax.set_title(\"Temporal Factors with Shading\")\n",
    "    ax.set_xlabel(\"Time Step\")\n",
    "    ax.set_ylabel(\"Activation\")\n",
    "\n",
    "    # if len(models) > 1:\n",
    "    #     ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "plot_temporal_factors(models, normalize_temporal=True)\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ef383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalize the data to have zero mean and unit variance.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data = data.reshape(-1, 1)  # Reshape data to 2D if it's 1D\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data.flatten()\n",
    "\n",
    "\n",
    "sorted_normalized_factors = []\n",
    "for model in models:\n",
    "    normalized_factors = normalize_data(model.factors[1][0, :])\n",
    "    indices = np.argsort(normalized_factors)[::-1]  # Sort by descending order\n",
    "    sorted_factors = normalized_factors[indices]\n",
    "    sorted_normalized_factors.append(sorted_factors)\n",
    "\n",
    "neuron_factors = np.vstack(sorted_normalized_factors)\n",
    "Z = linkage(neuron_factors, method=\"ward\")\n",
    "\n",
    "# Determine the clusters at a certain distance threshold\n",
    "cluster_labels = fcluster(Z, t=5, criterion=\"distance\")\n",
    "# Count number of models per cluster\n",
    "cluster_count = np.bincount(cluster_labels)\n",
    "# Find the most prevalent cluster\n",
    "most_prevalent_cluster = np.argmax(cluster_count[1:]) + 1\n",
    "\n",
    "# Filter models that belong to the most prevalent cluster\n",
    "cluster_models = [\n",
    "    models[i] for i in range(len(models)) if cluster_labels[i] == most_prevalent_cluster\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=[f\"Model {i+1}\" for i in range(len(models))],\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=12,\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering of Sorted Neuron Factors\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Distance (Ward's method)\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal factors for models in the most prevalent cluster\n",
    "plot_temporal_factors(cluster_models, normalize_temporal=True)\n",
    "\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_highly_active_neurons(neuron_factors, threshold=0.5):\n",
    "    \"\"\"Count the number of neurons that are above the activation threshold.\"\"\"\n",
    "    return (neuron_factors > threshold).sum(axis=1)\n",
    "\n",
    "\n",
    "# Normalize and sort the neuron factors\n",
    "sorted_normalized_neurons = []\n",
    "for model in models:\n",
    "    normalized_neurons = normalize_data(model.factors[1][0, :])\n",
    "    sorted_neurons = np.sort(normalized_neurons)[::-1]\n",
    "    sorted_normalized_neurons.append(sorted_neurons)\n",
    "\n",
    "# Stack and cluster\n",
    "neuron_factors = np.vstack(sorted_normalized_neurons)\n",
    "Z = linkage(neuron_factors, method=\"ward\")\n",
    "cluster_labels = fcluster(Z, t=5, criterion=\"distance\")\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_stats = {}\n",
    "for cluster_label in np.unique(cluster_labels):\n",
    "    indices = np.where(cluster_labels == cluster_label)[0]\n",
    "    cluster_neurons = neuron_factors[indices]\n",
    "    active_counts = count_highly_active_neurons(cluster_neurons)\n",
    "    train_accuracies = [accuracies.iloc[i][\"Train Accuracy\"] for i in indices]\n",
    "    test_accuracies = [accuracies.iloc[i][\"Test Accuracy\"] for i in indices]\n",
    "    cluster_stats[cluster_label] = {\n",
    "        \"models\": indices,\n",
    "        \"active_neuron_counts\": active_counts,\n",
    "        \"average_active_neurons\": np.mean(active_counts),\n",
    "        \"std_dev_active_neurons\": np.std(active_counts),\n",
    "        \"average_train_accuracy\": np.mean(train_accuracies),\n",
    "        \"average_test_accuracy\": np.mean(test_accuracies),\n",
    "        \"train_accuracies\": accuracies.iloc[indices][\"Train Accuracy\"].values,\n",
    "        \"test_accuracies\": accuracies.iloc[indices][\"Test Accuracy\"].values,\n",
    "    }\n",
    "\n",
    "# Output the results for analysis\n",
    "for label, stats in cluster_stats.items():\n",
    "    print(f\"Cluster {label}:\")\n",
    "    print(f\"  Number of models: {len(stats['models'])}\")\n",
    "    print(f\"  Average highly active neurons: {stats['average_active_neurons']:.2f}\")\n",
    "    print(f\"  Standard deviation: {stats['std_dev_active_neurons']:.2f}\")\n",
    "    print(f\"  Average Train Accuracy: {stats['average_train_accuracy']:.2f}%\")\n",
    "    print(f\"  Average Test Accuracy: {stats['average_test_accuracy']:.2f}%\")\n",
    "    print(f\"  Active counts per model: {stats['active_neuron_counts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8badac0c",
   "metadata": {},
   "source": [
    "New plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "positions = np.arange(len(cluster_stats))\n",
    "\n",
    "# Sort clusters based on average active neurons\n",
    "sorted_clusters = sorted(\n",
    "    cluster_stats.items(), key=lambda x: x[1][\"average_active_neurons\"]\n",
    ")\n",
    "\n",
    "# Plot train and test accuracies for each cluster\n",
    "for i, (cluster, stats) in enumerate(sorted_clusters):\n",
    "    train_accuracies = stats[\"train_accuracies\"]\n",
    "    test_accuracies = stats[\"test_accuracies\"]\n",
    "\n",
    "    jitter = np.random.normal(0, 0.05, size=len(train_accuracies))  # Reduced jitter\n",
    "\n",
    "    # training accuracies\n",
    "    ax.scatter(\n",
    "        positions[i] + jitter - 0.15,\n",
    "        train_accuracies,\n",
    "        color=\"dodgerblue\",\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        marker=\"o\",\n",
    "        edgecolors=\"black\",\n",
    "        label=\"Train Accuracy\" if i == 0 else \"\",\n",
    "    )\n",
    "\n",
    "    # testing accuracies\n",
    "    ax.scatter(\n",
    "        positions[i] + jitter + 0.15,\n",
    "        test_accuracies,\n",
    "        color=\"limegreen\",\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        marker=\"s\",\n",
    "        edgecolors=\"black\",\n",
    "        label=\"Test Accuracy\" if i == 0 else \"\",\n",
    "    )\n",
    "\n",
    "    # median values with a line\n",
    "    median_train = np.median(train_accuracies)\n",
    "    median_test = np.median(test_accuracies)\n",
    "    ax.plot(\n",
    "        [positions[i] - 0.15, positions[i] + 0.15],\n",
    "        [median_train, median_test],\n",
    "        \"k-\",\n",
    "        alpha=0.75,\n",
    "        marker=\"o\",\n",
    "        markersize=8,\n",
    "    )\n",
    "\n",
    "# Styling\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([f\"Cluster {cluster}\" for cluster in cluster_stats.keys()])\n",
    "# ax.set_xticklabels([f\"Cluster {cluster}\" for cluster, _ in sorted_clusters])\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "# ax.set_title(\"Training and Test Accuracies by Cluster\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(str(fig_counter) + '.png', dpi=300)\n",
    "plt.savefig(str(fig_counter) + '.tiff', dpi=300)\n",
    "fig_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
