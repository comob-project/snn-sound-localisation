Inspired by the success of endeavours like the [Human Genome Project](https://www.genome.gov/human-genome-project) and [CERN](https://home.cern/), neuroscientists are increasingly initiating large-scale collaborations. Though, how to best structure these projects remains an open-question {cite:p}`doi.org/10.1038/539159a`. The largest efforts, e.g. the [International Brain Laboratory](https://www.internationalbrainlab.com/) [@doi.org/10.1016/j.neuron.2017.12.013;@doi.org/10.1016/j.conb.2020.10.020], [The Blue Brain Project](https://www.epfl.ch/research/domains/bluebrain/) and [Human Brain Project](https://www.humanbrainproject.eu) bring together tens to hundreds of researchers across multiple laboratories. However, while these projects represent a step-change in scale, they retain a legacy structure which resembles a consortia grant. I.e. there are participating laboratories who collaborate together and then make their data, methods and results available upon publication. As such, interested participants face a high barrier to entry: joining a participating laboratory, initiating a collaboration with the project, or awaiting publications. So how could these projects be structured differently?   

One alternative is a bench marking contest, in which participants compete to obtain the best score on a specific task. Such contests have driven progress in fields from machine learning {cite:p}`10.1109/CVPR.2009.5206848` to [protein folding](https://predictioncenter.org/), and have begun to enter neuroscience. For example, in [Brain-Score](https://www.brain-score.org/) [@10.1101/407007;@10.1016/j.neuron.2020.07.040] participants submit models, capable of completing a visual processing task, which are then ranked according to a quantitative metric. As participants can compete both remotely and independently, these contests offer a significantly lower barrier to entry. Though, they emphasise competition over collaboration, and critically they require a well defined, quantifiable endpoint. In [Brain-Score](https://www.brain-score.org/), this endpoint is a composite metric which describes the model's similarity to experimental data in terms of both behaviour and unit activity. However, this metric's relevance is debatable {cite:p}`doi:10.1017/S0140525X22002813` and more broadly, defining clear endpoints for neuroscientific questions remains challenging.    

Another alternative is massively collaborative projects in which participants work together to solve a common goal. For example, in the [Polymath Project](https://polymathprojects.org/) unsolved mathematical problems are posed, and then participants share comments, ideas and equations online as they collectively work towards solutions. Inspired by this approach, we founded [COMOB (Collaborative Modelling of the Brain)](https://comob-project.github.io/) - an open-source movement, which aims to tackle neuroscientific questions. Here, we share our experiences and results from our first project, in which we explored spiking neural network models of sound localization.

We start by detailing how we ran the project both in terms of infrastructure and organisationally in [](#metascience). We then briefly summarise the scientific results in [](#science). We conclude the main text with a [](#discussion) of what went well, what went wrong, and how we think future projects of this sort could learn from our experiences. Finally, in the [](#appendices) we give longer more detailed write-ups of some of the more detailed scientific results.