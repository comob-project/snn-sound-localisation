## Workflow
Our project grew out of a tutorial at the [Cosyne conference](https://www.cosyne.org/) (2022) for which we provided [video lectures and code online](https://neural-reckoning.github.io/cosyne-tutorial-2022/) {cite:p}`10.5281/zenodo.7044500`. Participants joining the project were encouraged to review this material, and then to work through an introductory Jupyter Notebook {cite:p}`Kluyver2016jupyter` containing Python code, figures and markdown text, which taught them how to train a spiking neural network to perform a sound localisation task. Participants were then directed to our website where we maintained a list of open scientific and technical questions for inspiration. For example, how does the use of different neuron models impact network performance and can we learn input delays with gradient descent? Then, with a proposed or novel question in hand, participants were free to approach their work as they wished. In practice, much like a "typical" research project, most work was conducted individually, shared at monthly online meetings and then iteratively improved upon. For example, several early career researchers tackled questions full-time as their dissertation or thesis work and benefited from external input at monthly workshops.

We consciously decided on this free-form structure to experiment with the feasibility of a more bottom-up approach to doing team science, with minimal top-down supervision. We discuss the advantages and disadvantages of this approach in the [](#discussion).

## Infrastructure

### Code
We provided a [](../research/3-Starting-Notebook.ipynb) to give participants an easy way to get started. This is a Python-based Jupyter Notebook {cite:p}`Kluyver2016jupyter`, an interactive cell-based environment which allows a mixture of text, image and code cells to be weaved together, combined with an easy user interface. Participants could either work locally using their own Python distribution, or using a cloud compute service such as [Google Colab](https://colab.research.google.com/). We choose Google Colab to minimise the entry barrier for participation, as it is a free service (although blocked in certain countries unfortunately) where all the software packages needed are pre-installed, meaning it takes users only a few seconds to go from reading about the project to running the code.

Our starting notebook used a combination of NumPy [@harris2020array], Matplotlib [@Hunter2007], and PyTorch [@paszke_pytorch_2019]. The code for surrogate gradient descent was based on Friedemann Zenke's SPyTorch tutorial [@zenke_spytorch_2019;@Zenke2018].

Note that we didn't require participants to use our starting notebook, and indeed in [](#inhib-model), De Santis and Antonietti implemented a very different sound localization model from scratch.

### GitHub
Like many open-source efforts, [our public GitHub repository](https://github.com/comob-project/snn-sound-localization) was the heart of our project. This provided us with three main benefits. First, it made joining the project as simple as cloning and committing to the repository. Second, it allowed us to collaborate asynchronously. That is, we could easily complete work in our own time, and then share it with the group later. Third, it allowed us to track contributions to the project. Measured in this way, 26 individuals contributed to the project. However, interpreting this number is challenging, as these contributions vary significantly in size, and participants who worked in pairs or small groups, often contributed under a single username. We return to this point in the [](#discussion).

### Website via MyST Markdown
For those interested in pursuing a similar project our repository can easily be used as a template. It consists of a collection of documents written in Markdown and executable [Jupyter Notebooks](https://jupyter.org/) {cite:p}`Kluyver2016jupyter` containing all the code for the project. Each time the repository is updated, GitHub automatically builds these documents and notebooks into a website so that the current state of the project can be seen by simply navigating to the [project website](https://comob-project.github.io/snn-sound-localization). We used [MyST Markdown](https://mystmd.org/) to automate this process with minimal effort. This paper itself was written using these tools and was publicly visible throughout the project write-up.
