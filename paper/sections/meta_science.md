## Workflow
Our project grew out of a tutorial at the [Cosyne conference](https://www.cosyne.org/) (2022) for which we provided [video lectures and code online](https://neural-reckoning.github.io/cosyne-tutorial-2022/) {cite:p}`10.5281/zenodo.7044500`. Participants joining the project were encouraged to review this material, and then to work through an introductory Jupyter Notebook {cite:p}`Kluyver2016jupyter` containing Python code, figures and markdown text, which taught them how to train a spiking neural network to perform a sound localisation task. Participants were then directed to our website where we maintained a list of open scientific and technical questions for inspiration. For example, how does the use of different neuron models impact network performance and can we learn input delays with gradient descent? Then, with a proposed or novel question in hand, participants were free to approach their work as they wished. In practice, much like a "typical" research project, most work was conducted individually, shared at monthly online meetings and then iteratively improved upon. For example, several early career researchers tackled questions full-time as their dissertation or thesis work and benefited from external input at monthly workshops.

```{figure} ./sections/_figures/COMOB_workflow.png
:label: COMOB_workflow

Our workflow. To on-board new participants we provided text, videos and code outlining our scientific starting point. This material formed a springboard for participants to pursue research either individually or in small groups. We then iteratively improved on this work through monthly online workshops and by writing this paper together through an open, collaborative process.      
```

We consciously decided on this free-form structure to experiment with the feasibility of a more bottom-up approach to doing team science, with minimal top-down supervision. We discuss the advantages and disadvantages of this approach in the [](#discussion).

## Infrastructure

### Code
We provided a [](../research/3-Starting-Notebook.ipynb) to give participants an easy way to get started. This is a Python-based Jupyter Notebook {cite:p}`Kluyver2016jupyter`, an interactive cell-based environment which allows a mixture of text, image and code cells to be weaved together, combined with an easy user interface. Participants could either work locally using their own Python distribution, or using a cloud compute service such as [Google Colab](https://colab.research.google.com/). We choose Google Colab to minimise the entry barrier for participation, as it is a free service (although blocked in certain countries unfortunately) where all the software packages needed are pre-installed, meaning it takes users only a few seconds to go from reading about the project to running the code.

Our starting notebook used a combination of NumPy [@harris2020array], Matplotlib [@Hunter2007], and PyTorch [@paszke_pytorch_2019]. The code for surrogate gradient descent was based on Friedemann Zenke's SPyTorch tutorial [@zenke_spytorch_2019;@Zenke2018].

Note that we didn't require participants to use our starting notebook, and indeed in [](#inhib-model), De Santis and Antonietti implemented a very different sound localization model from scratch.

### GitHub
Like many open-source efforts, [our public GitHub repository](https://github.com/comob-project/snn-sound-localization) was the heart of our project. This provided us with three main benefits. First, it made joining the project as simple as cloning and committing to the repository. Second, it allowed us to collaborate asynchronously. That is, we could easily complete work in our own time, and then share it with the group later. Third, it allowed us to track contributions to the project. Measured in this way, 28 individuals contributed to the project. However, interpreting this number is challenging, as these contributions vary significantly in size, and participants who worked in pairs or small groups, often contributed under a single username. We return to this point in the [](#discussion).

### Website via MyST Markdown
For those interested in pursuing a similar project our repository can easily be used as a template. It consists of a collection of documents written in Markdown and executable [Jupyter Notebooks](https://jupyter.org/) {cite:p}`Kluyver2016jupyter` containing all the code for the project. Each time the repository is updated, GitHub automatically builds these documents and notebooks into a website so that the current state of the project can be seen by simply navigating to the [project website](https://comob-project.github.io/snn-sound-localization). We used [MyST Markdown](https://mystmd.org/) to automate this process with minimal effort. This paper itself was written using these tools and was publicly visible throughout the project write-up.

(teaching-section)=
## Teaching with this framework
As our code uses spiking neurons to transform sensory inputs into behavioural outputs, it forms an excellent basis for teaching, as concepts from across neuroscience can be introduced and then implemented in class. With this in mind we integrated our project into physics M1 and M2 undergraduate lectures on biophysics and neural circuits. Working individually or in pairs, students actively engaged by adjusting network parameters and modifying the provided code to test their own hypotheses. Later, brief progress report presentations stimulated dynamic discussions in class, as all students, while working on the same project and code, pursued different hypotheses. This setup naturally piqued interest in their peers’ presentations, enhanced their understanding of various project applications, and facilitated collaborative learning.

The project’s stochastic outcomes necessitated substantial statistical analysis, adding an experimental dimension that made the project outcome less deterministic and, thus, more engaging than standard step-wise exercises. However, the project does not demand complex programming nor deep mathematical understandings of neural networks, and so allows practical exploration of neural network applications appropriate for various student levels. This adaptability allowed students of varying skill levels to progress at their own pace. Moreover, the open-ended nature of the project allowed the use of generative AI tools, enabling students to overcome coding challenges and deepen their understanding of the provided code and underlying machine learning concepts, thereby enhancing their learning curve and engagement.

Working on a real research project not only sustained interest and demonstrated real-world impact but also provided additional inspiration through the accessible contributions of all project participants. This educational initiative thus successfully bridged the gap between teaching and research, with student feedback highlighting its effectiveness in enhancing both theoretical and practical knowledge. The desire for more time to delve deeper into the projects indicated its strength in engaging students and sparking their interest.

In sum, this framework's multidisciplinary nature makes it versatile in various teaching contexts, and suited to discussing both machine learning concepts and open challenges in neuroscience, such as how to decipher brain circuits with recording tools and experimental manipulations like optogenetics. For those interested in teaching with this framework, we have provided slides and a highly annotated introductory Python notebook [here](). 