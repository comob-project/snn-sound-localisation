# Results & Discussion [WIP]

First, different hyperparameter configurations were investigated for the **baseline SNN model** in order to establish a baseline performance for comparison. The baseline SNN was trained and tested in three major settings: **(1)** where the phase delays are used at input spike generation (for both training and testing); **(2)** where there are no phase delays at input spike generation (for both training and testing); **(3)** where the model is trained on training input with phase delays, but tested on test input without phase delays. The third setting is particularly interesting, as it provides a means of checking whether the model makes use of the available information about delays to solve the task. Accuracy results for settings **(1)**, **(2)** and **(3)** are shown in Table X. In the original setting — **setting (1)** —, the performance of the SNN is consistently above chance level and generalizable (provided a sensible learning rate), with the model exhibiting high classification capabilities across training and test sets. The same observation can be made in **setting (2)** for certain values of the training learning rate, suggesting that the baseline SNN model can learn to solve the task without information about the phase delays between the sound signals that come from two different sources (left ear - right ear). Then, does such a model make use of that information at all to solve the task? Results from **setting (3)** confirm that, when available, information about phase delays are used by the model to solve the task: indeed, the consistent drop in accuracy from training to testing is more significant in this setting than the previous ones. As the neural network's capacity increases, this drop in performance may become less noticeable or even avoidable (**setting (4)**): to test this hypothesis, we ran the same configuration as in **setting (3)**, but with a varying number ($n\_hidden$) of hidden neurons (see results for **setting (4)** in Table X). Results for **setting (4)** suggest that increasing the capacity of the network (i.e., increasing $num\_hidden$) does not substantially help the network's generalisability when the network has been trained with delay information that is unavailable at test time.

**Experiment 1** revealed that the high performance (accuracy) of the baseline SNN on the classification task can be maintained in certain configurations (e.g., depending on the learning rate, or membrane time constants), when simple synaptic dynamics are introduced. That is, in this fixed synaptic dynamics setup, there were specific rise and decay time constant pairings that led to no drop in task accuracy compared to the best performance results reported for the baseline SNN. Importantly, this observation was made in two separate settings: **(1)** where the phase delays present in the input spike trains are kept (see Figure X; e.g. $tau_{rise} = 1.4$, $tau_{decay} = 1.6$); **(2)** where those phase delays were completely ignored during input dataset generation (see Figure X; e.g. $tau_{rise} = 1.0$, $tau_{decay} = 1.6$). Still, an overall decrease in (training and test) classification accuracy — across rise and decay time constant pairings — can be noted in the absence of delay information.

We hypothesized that the setup introduced in **Experiment 2** could prevent this decrease of performance in the absence of delay information, thanks to the temporal dynamics introduced by the heterogeneity in the rise and decay time constants used across "synapses" (i.e., connections in the neural network). Table X summarizes the training and test accuracy results obtained for 5 random seeds per experiment; an experiment is defined by the learning rate used (10 numerical values tested: see Table 1) and whether the delays are used during both training and testing (Boolean value). The network's performance was substantially variable across random seeds of the same configurations. The choice of learning rate significantly impacted the seed-averaged classification accuracies; the impact on task performance also depended on whether delays were allowed at training and test set generation. Overall, in both delay settings, the performance of the network was hindered compared to previous experiments, including **Experiment 1** without delays and **baseline SNN experiments**. Still, we observed that the network's performance with such heterogeneous synaptic dynamics did not consistently or strongly change with the removal of delay information. This observation suggests that **(1)** the temporal dynamics introduced with synaptic filtering may somewhat compensate for the temporal information conveyed with signal delays; **(2)** and the overall drop in performance in **Experiment 2** is due to the loss of temporal precision in the information propagated through the network (given the synaptic filtering applied).
